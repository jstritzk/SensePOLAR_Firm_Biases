{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f02cc38",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cda79c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet as wn\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df44f5dd",
   "metadata": {},
   "source": [
    "### 1. Process Seed Word Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fcffa0",
   "metadata": {},
   "source": [
    "Processing the seed word list from: \n",
    "\n",
    "- *Nicolas, G., Bai, X., & Fiske, S. T. (2021). Comprehensive stereotype content dictionaries using a semi‐automated method. In European Journal of Social Psychology (Vol. 51, Issue 1, pp. 178–196). Wiley. https://doi.org/10.1002/ejsp.2724* \n",
    "\n",
    "which is included in the file *finalwordlist R 050418.csv* in the online supplement found under https://osf.io/yx45f/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e3a0655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read seed word list. \n",
    "seed_words = pd.read_csv(\"finalwordlist R 050418.csv\")\n",
    "seed_words = seed_words[seed_words[\"PoS\"] == \"ADJECTIVE\"]\n",
    "\n",
    "\n",
    "# Add relevant WordNet information. \n",
    "def get_synset(row):\n",
    "    return wn.synsets(row[\"term\"], pos = wn.ADJ)[row[\"sense\"]-1].name()\n",
    "\n",
    "def get_definition(row):\n",
    "    return wn.synsets(row[\"term\"], pos = wn.ADJ)[row[\"sense\"]-1].definition()\n",
    "\n",
    "def get_example(row):\n",
    "    try: \n",
    "        return wn.synsets(row[\"term\"], pos = wn.ADJ)[row[\"sense\"]-1].examples()[0]\n",
    "    except:\n",
    "        pass\n",
    "      \n",
    "def get_antonyms(row):\n",
    "    antonyms = []\n",
    "    for lemma in wn.synsets(row[\"term\"], pos = wn.ADJ)[row[\"sense\"]-1].lemmas():\n",
    "        try: \n",
    "            for antonym in lemma.antonyms(): \n",
    "                antonyms.append(antonym.synset().name())\n",
    "        except: \n",
    "            pass\n",
    "    try: \n",
    "        return antonyms[0]\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "# Applying functions. \n",
    "seed_words[\"Synset\"] = seed_words.apply(lambda row: get_synset(row), axis=1)\n",
    "seed_words[\"Definition\"] = seed_words.apply(lambda row: get_definition(row), axis=1)\n",
    "seed_words[\"Examples\"] = seed_words.apply(lambda row: get_example(row), axis=1)\n",
    "seed_words[\"Antonyms\"] = seed_words.apply(lambda row: get_antonyms(row), axis=1)\n",
    "\n",
    "# Create individual dictionaries. \n",
    "sociability_seed = seed_words[seed_words[\"Dictionary\"] == \"Sociability\"]\n",
    "ability_seed = seed_words[seed_words[\"Dictionary\"] == \"Ability\"]\n",
    "status_seed = seed_words[seed_words[\"Dictionary\"] == \"Status\"]\n",
    "agency_seed = seed_words[seed_words[\"Dictionary\"] == \"Agency\"]\n",
    "morality_seed = seed_words[seed_words[\"Dictionary\"] == \"Morality\"]\n",
    "politics_seed = seed_words[seed_words[\"Dictionary\"] == \"Politics\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d3cf1d",
   "metadata": {},
   "source": [
    "### 2. Match Antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "543c8fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_match_antonyms(df):\n",
    "    \n",
    "    # Split dictionary into high and low and remove dimensions with missing matches. \n",
    "    dict_high = df[(df[\"Dir\"] == \"high\") & (df[\"Antonyms\"].notnull())]\n",
    "    dict_low = df[(df[\"Dir\"] == \"low\") & (df[\"Antonyms\"].notnull())]\n",
    "    \n",
    "    # Clean and rename columns. \n",
    "    dict_high = dict_high[[\"term\", \"Synset\", \"Definition\", \"Examples\", \"Antonyms\"]]\n",
    "    dict_high = dict_high.rename(columns={\"term\":\"Term1\", \"Synset\": 'Synset1', \"Definition\": \"Definition1\", \n",
    "                                          \"Examples\":\"Example1\", \"Antonyms\": \"Synset2\"})\n",
    "    \n",
    "    dict_low = dict_low[[\"term\", \"Synset\", \"Definition\", \"Examples\"]]\n",
    "    dict_low = dict_low.rename(columns={\"term\":\"Term2\", \"Synset\": 'Synset2', \"Definition\": \"Definition2\", \n",
    "                                       \"Examples\": \"Example2\"})\n",
    "    \n",
    "    # Merge both dictionaries. \n",
    "    dict_merged = pd.merge(dict_high, dict_low, on=[\"Synset2\"])\n",
    "    dict_merged = dict_merged.dropna().reset_index(drop = True)\n",
    "    dict_merged.drop_duplicates(inplace = True)\n",
    "\n",
    "    return dict_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49674621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and match antonym dictionaries \n",
    "\n",
    "sociability_seed_matched = clean_and_match_antonyms(sociability_seed)\n",
    "ability_seed_matched = clean_and_match_antonyms(ability_seed)\n",
    "status_seed_matched = clean_and_match_antonyms(status_seed)\n",
    "agency_seed_seed_matched = clean_and_match_antonyms(agency_seed)\n",
    "morality_seed_matched = clean_and_match_antonyms(morality_seed)\n",
    "politics_seed_matched = clean_and_match_antonyms(politics_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f2801ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sociability_seed_matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eedeb5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ability_seed_matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6b104d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(status_seed_matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a64a153f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agency_seed_seed_matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81ea5734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(morality_seed_matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16829dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(politics_seed_matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba92b45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "656ea6e5",
   "metadata": {},
   "source": [
    "### 3. Create Files compatible with SensePOLAR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f1d181",
   "metadata": {},
   "source": [
    "To be compatible with the SensePolar framework, seed word lists are converted into different text and pkl files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c352260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_antonym_text_files(df):\n",
    "\n",
    "    example_sentences_readable = {}\n",
    "    get_lookup_anto_example_dict = []\n",
    "    get_lookup_synset_definition = []\n",
    "    get_lookup_synset_dict = []\n",
    "    \n",
    "    for entry in tqdm(df.index):\n",
    "        \n",
    "        # Create antonym_wordnet_example_sentences_readable_extended.txt\n",
    "        antonym_pair = str([str(df.at[entry, \"Synset1\"]), str(df.at[entry, \"Synset2\"])])\n",
    "        \n",
    "        term1 = df.at[entry, \"Term1\"]\n",
    "        term2 = df.at[entry, \"Term2\"]        \n",
    "        example1 = df.at[entry, \"Example1\"]\n",
    "        example2 = df.at[entry, \"Example2\"]\n",
    "        \n",
    "        sub_dict = {term1:[example1], term2:[example2]}\n",
    "        \n",
    "        example_sentences_readable[antonym_pair] = sub_dict\n",
    "    \n",
    "        \n",
    "        # Create get_lookup_anto_example_dict. \n",
    "        \n",
    "        get_lookup_anto_example_dict.append([[term1, [example1]], [term2, [example2]]])\n",
    "        \n",
    "        \n",
    "        # Create get_lookup_synset_definition. \n",
    "        definition1 = df.at[entry, \"Definition1\"]\n",
    "        definition2 = df.at[entry, \"Definition2\"]\n",
    "        get_lookup_synset_definition.append([definition1, definition2])\n",
    "        \n",
    "        # Create get_lookup_synset_dict. \n",
    "        get_lookup_synset_dict.append(antonym_pair)\n",
    "\n",
    "    # Create text files. \n",
    "    with open('antonyms/antonym_wordnet_example_sentences_readable_extended.txt', 'w') as file:\n",
    "        file.write(json.dumps(example_sentences_readable, indent=4))\n",
    "    with open('antonyms/lookup_anto_example_dict.txt', 'w') as file:\n",
    "        file.write(json.dumps(get_lookup_anto_example_dict, indent=4))\n",
    "    with open('antonyms/lookup_synset_definition.txt', 'w') as file:\n",
    "        file.write(json.dumps(get_lookup_synset_definition, indent=4))        \n",
    "    with open('antonyms/lookup_synset_dict.txt', 'w') as file:\n",
    "        file.write(json.dumps(get_lookup_synset_dict, indent=4))\n",
    "        \n",
    "    # Create pkl files.    \n",
    "    with open('antonyms/lookup_anto_example_dict.pkl', 'wb') as file:\n",
    "        pickle.dump(get_lookup_anto_example_dict, file)\n",
    "    with open('antonyms/lookup_synset_definition.pkl', 'wb') as file:\n",
    "        pickle.dump(get_lookup_synset_definition, file)\n",
    "    with open('antonyms/lookup_synset_dict.pkl', 'wb') as file:\n",
    "        pickle.dump(get_lookup_synset_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27225166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d05b5a9c7d064250ae3c92a433c62348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_antonym_text_files(ability_seed_matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292fd496",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
